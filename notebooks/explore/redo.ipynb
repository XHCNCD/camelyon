{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo everything on 20% sample, training everything in most basic way possible\n",
    "* First fine tune some models , then do fusion\n",
    "* Maybe try fully fine-tuning networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101, vgg16\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Set it to use GPU1\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make 20% sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_sample_dataset(data_loc, new_loc, ttv_folders, frac):\n",
    "#     \"\"\"ttv_folders are the names of test, train, valid folders. Assume already in fastai folder format\"\"\"   \n",
    "#     for folder in ttv_folders:\n",
    "#         classes = [name for name in os.listdir(os.path.join(data_loc, folder)) if os.path.isdir(os.path.join(data_loc, folder, name))]\n",
    "#         # go through classes, make output directory, copy a sample of image to this\n",
    "#         for class_name in classes:\n",
    "#             curr_path = os.path.join(data_loc, folder, class_name)\n",
    "#             out_path = os.path.join(new_loc, folder, class_name)\n",
    "#             if not os.path.exists(out_path):\n",
    "#                 os.makedirs(out_path)\n",
    "                \n",
    "#             files = glob.glob(curr_path + '/*.png')\n",
    "#             sample_files = random.sample(files, int(len(files)*frac))\n",
    "#             for file_to_copy in sample_files:\n",
    "#                 shutil.copyfile(file_to_copy, os.path.join(new_loc, folder, class_name, file_to_copy.rsplit('/')[-1]))\n",
    "\n",
    "# PATH = '/media/rene/Data/camelyon_out/tiles_224_100t'\n",
    "# new_loc = os.path.join(PATH, 'sample')\n",
    "# ttv_folders = ['train', 'valid']\n",
    "# frac = .2\n",
    "# make_sample_dataset(PATH, new_loc, ttv_folders, frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    use_gpu = True\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # for nets that have multiple outputs such as inception\n",
    "                if isinstance(outputs, tuple):\n",
    "                    loss = sum((criterion(o,labels) for o in outputs))\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0] * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            # stop those memory leaks\n",
    "            del loss, outputs \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best valid Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return best_acc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_gen(PATH, batch_size, num_workers, valid_name='valid'):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        valid_name: transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(PATH, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', valid_name]}\n",
    "\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                                 shuffle=True, num_workers=num_workers)\n",
    "                  for x in ['train', valid_name]}\n",
    "\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', valid_name]}\n",
    "    return dataloaders, dataset_sizes\n",
    "\n",
    "\n",
    "PATH = '/media/rene/Data/camelyon_out/tiles_224_100t/sample'\n",
    "num_workers = 4\n",
    "batch_size=32\n",
    "sz=224\n",
    "\n",
    "dataloaders, dataset_sizes = make_batch_gen(PATH, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/11\n",
      "----------\n",
      "train Loss: 0.2811 Acc: 0.8881\n",
      "valid Loss: 0.2075 Acc: 0.9210\n",
      "Epoch 1/11\n",
      "----------\n",
      "train Loss: 0.2397 Acc: 0.9029\n",
      "valid Loss: 0.3004 Acc: 0.8659\n",
      "Epoch 2/11\n",
      "----------\n",
      "train Loss: 0.2259 Acc: 0.9065\n",
      "valid Loss: 0.2160 Acc: 0.9120\n",
      "Epoch 3/11\n",
      "----------\n",
      "train Loss: 0.1888 Acc: 0.9196\n",
      "valid Loss: 0.1785 Acc: 0.9269\n",
      "Epoch 4/11\n",
      "----------\n",
      "train Loss: 0.1778 Acc: 0.9270\n",
      "valid Loss: 0.1804 Acc: 0.9278\n",
      "Epoch 5/11\n",
      "----------\n",
      "train Loss: 0.1714 Acc: 0.9299\n",
      "valid Loss: 0.1615 Acc: 0.9341\n",
      "Epoch 6/11\n",
      "----------\n",
      "train Loss: 0.1630 Acc: 0.9343\n",
      "valid Loss: 0.1701 Acc: 0.9320\n",
      "Epoch 7/11\n",
      "----------\n",
      "train Loss: 0.1578 Acc: 0.9359\n",
      "valid Loss: 0.1648 Acc: 0.9320\n",
      "Epoch 8/11\n",
      "----------\n",
      "train Loss: 0.1568 Acc: 0.9370\n",
      "valid Loss: 0.1715 Acc: 0.9311\n",
      "Epoch 9/11\n",
      "----------\n",
      "train Loss: 0.1559 Acc: 0.9382\n",
      "valid Loss: 0.1689 Acc: 0.9311\n",
      "Epoch 10/11\n",
      "----------\n",
      "train Loss: 0.1537 Acc: 0.9380\n",
      "valid Loss: 0.1661 Acc: 0.9311\n",
      "Epoch 11/11\n",
      "----------\n",
      "train Loss: 0.1544 Acc: 0.9370\n",
      "valid Loss: 0.1633 Acc: 0.9314\n",
      "Training complete in 7m 46s\n",
      "Best valid Acc: 0.934132\n",
      "Epoch 0/11\n",
      "----------\n",
      "train Loss: 0.2846 Acc: 0.8844\n",
      "valid Loss: 0.2034 Acc: 0.9192\n",
      "Epoch 1/11\n",
      "----------\n",
      "train Loss: 0.2391 Acc: 0.9025\n",
      "valid Loss: 0.2038 Acc: 0.9210\n",
      "Epoch 2/11\n",
      "----------\n",
      "train Loss: 0.2215 Acc: 0.9096\n",
      "valid Loss: 0.2009 Acc: 0.9159\n",
      "Epoch 3/11\n",
      "----------\n",
      "train Loss: 0.1911 Acc: 0.9207\n",
      "valid Loss: 0.1802 Acc: 0.9257\n",
      "Epoch 4/11\n",
      "----------\n",
      "train Loss: 0.1849 Acc: 0.9242\n",
      "valid Loss: 0.1678 Acc: 0.9287\n",
      "Epoch 5/11\n",
      "----------\n",
      "train Loss: 0.1775 Acc: 0.9281\n",
      "valid Loss: 0.1772 Acc: 0.9290\n",
      "Epoch 6/11\n",
      "----------\n",
      "train Loss: 0.1668 Acc: 0.9333\n",
      "valid Loss: 0.1718 Acc: 0.9293\n",
      "Epoch 7/11\n",
      "----------\n",
      "train Loss: 0.1617 Acc: 0.9357\n",
      "valid Loss: 0.1718 Acc: 0.9302\n",
      "Epoch 8/11\n",
      "----------\n",
      "train Loss: 0.1598 Acc: 0.9362\n",
      "valid Loss: 0.1705 Acc: 0.9296\n",
      "Epoch 9/11\n",
      "----------\n",
      "train Loss: 0.1578 Acc: 0.9373\n",
      "valid Loss: 0.1706 Acc: 0.9302\n",
      "Epoch 10/11\n",
      "----------\n",
      "train Loss: 0.1576 Acc: 0.9379\n",
      "valid Loss: 0.1726 Acc: 0.9278\n",
      "Epoch 11/11\n",
      "----------\n",
      "train Loss: 0.1601 Acc: 0.9358\n",
      "valid Loss: 0.1698 Acc: 0.9293\n",
      "Training complete in 13m 21s\n",
      "Best valid Acc: 0.930240\n",
      "Epoch 0/11\n",
      "----------\n",
      "train Loss: 0.2828 Acc: 0.8876\n",
      "valid Loss: 0.2601 Acc: 0.9000\n",
      "Epoch 1/11\n",
      "----------\n",
      "train Loss: 0.2408 Acc: 0.9038\n",
      "valid Loss: 0.2146 Acc: 0.9159\n",
      "Epoch 2/11\n",
      "----------\n",
      "train Loss: 0.2348 Acc: 0.9060\n",
      "valid Loss: 0.2083 Acc: 0.9087\n",
      "Epoch 3/11\n",
      "----------\n",
      "train Loss: 0.2049 Acc: 0.9173\n",
      "valid Loss: 0.1927 Acc: 0.9228\n",
      "Epoch 4/11\n",
      "----------\n",
      "train Loss: 0.1911 Acc: 0.9236\n",
      "valid Loss: 0.1896 Acc: 0.9222\n",
      "Epoch 5/11\n",
      "----------\n",
      "train Loss: 0.1867 Acc: 0.9227\n",
      "valid Loss: 0.1856 Acc: 0.9257\n",
      "Epoch 6/11\n",
      "----------\n",
      "train Loss: 0.1768 Acc: 0.9297\n",
      "valid Loss: 0.1748 Acc: 0.9305\n",
      "Epoch 7/11\n",
      "----------\n",
      "train Loss: 0.1732 Acc: 0.9310\n",
      "valid Loss: 0.1767 Acc: 0.9302\n",
      "Epoch 8/11\n",
      "----------\n",
      "train Loss: 0.1723 Acc: 0.9304\n",
      "valid Loss: 0.1822 Acc: 0.9275\n",
      "Epoch 9/11\n",
      "----------\n",
      "train Loss: 0.1655 Acc: 0.9343\n",
      "valid Loss: 0.1800 Acc: 0.9263\n",
      "Epoch 10/11\n",
      "----------\n",
      "train Loss: 0.1689 Acc: 0.9327\n",
      "valid Loss: 0.1820 Acc: 0.9263\n",
      "Epoch 11/11\n",
      "----------\n",
      "train Loss: 0.1705 Acc: 0.9324\n",
      "valid Loss: 0.1810 Acc: 0.9266\n",
      "Training complete in 20m 36s\n",
      "Best valid Acc: 0.930539\n",
      "Epoch 0/11\n",
      "----------\n",
      "train Loss: 0.3065 Acc: 0.8750\n",
      "valid Loss: 0.2398 Acc: 0.8964\n",
      "Epoch 1/11\n",
      "----------\n",
      "train Loss: 0.2478 Acc: 0.8985\n",
      "valid Loss: 0.2292 Acc: 0.9126\n",
      "Epoch 2/11\n",
      "----------\n",
      "train Loss: 0.2377 Acc: 0.9048\n",
      "valid Loss: 0.2597 Acc: 0.9009\n",
      "Epoch 3/11\n",
      "----------\n",
      "train Loss: 0.2091 Acc: 0.9154\n",
      "valid Loss: 0.2063 Acc: 0.9150\n",
      "Epoch 4/11\n",
      "----------\n",
      "train Loss: 0.2019 Acc: 0.9158\n",
      "valid Loss: 0.1963 Acc: 0.9219\n",
      "Epoch 5/11\n",
      "----------\n",
      "train Loss: 0.1942 Acc: 0.9213\n",
      "valid Loss: 0.2091 Acc: 0.9159\n",
      "Epoch 6/11\n",
      "----------\n",
      "train Loss: 0.1846 Acc: 0.9241\n",
      "valid Loss: 0.1982 Acc: 0.9207\n",
      "Epoch 7/11\n",
      "----------\n",
      "train Loss: 0.1825 Acc: 0.9268\n",
      "valid Loss: 0.1958 Acc: 0.9186\n",
      "Epoch 8/11\n",
      "----------\n",
      "train Loss: 0.1756 Acc: 0.9281\n",
      "valid Loss: 0.1957 Acc: 0.9192\n",
      "Epoch 10/11\n",
      "----------\n",
      "train Loss: 0.1781 Acc: 0.9273\n",
      "valid Loss: 0.1900 Acc: 0.9228\n",
      "Epoch 11/11\n",
      "----------\n",
      "train Loss: 0.1767 Acc: 0.9292\n",
      "valid Loss: 0.1923 Acc: 0.9204\n",
      "Training complete in 34m 59s\n",
      "Best valid Acc: 0.922754\n",
      "Epoch 0/11\n",
      "----------\n",
      "train Loss: 0.3655 Acc: 0.8527\n",
      "valid Loss: 0.2747 Acc: 0.8749\n",
      "Epoch 1/11\n",
      "----------\n",
      "train Loss: 0.2829 Acc: 0.8834\n",
      "valid Loss: 0.2286 Acc: 0.9063\n",
      "Epoch 2/11\n",
      "----------\n",
      "train Loss: 0.2617 Acc: 0.8941\n",
      "valid Loss: 0.2522 Acc: 0.8982\n",
      "Epoch 3/11\n",
      "----------\n",
      "train Loss: 0.2256 Acc: 0.9082\n",
      "valid Loss: 0.2173 Acc: 0.9162\n",
      "Epoch 4/11\n",
      "----------\n",
      "train Loss: 0.2213 Acc: 0.9106\n",
      "valid Loss: 0.2130 Acc: 0.9171\n",
      "Epoch 5/11\n",
      "----------\n",
      "train Loss: 0.2162 Acc: 0.9149\n",
      "valid Loss: 0.2121 Acc: 0.9156\n",
      "Epoch 6/11\n",
      "----------\n",
      "train Loss: 0.2103 Acc: 0.9150\n",
      "valid Loss: 0.2212 Acc: 0.9156\n",
      "Epoch 7/11\n",
      "----------\n",
      "train Loss: 0.2093 Acc: 0.9159\n",
      "valid Loss: 0.2142 Acc: 0.9177\n",
      "Epoch 8/11\n",
      "----------\n",
      "train Loss: 0.2081 Acc: 0.9164\n",
      "valid Loss: 0.2168 Acc: 0.9186\n",
      "Epoch 9/11\n",
      "----------\n",
      "train Loss: 0.2067 Acc: 0.9163\n",
      "valid Loss: 0.2166 Acc: 0.9189\n",
      "Epoch 10/11\n",
      "----------\n",
      "train Loss: 0.2060 Acc: 0.9178\n",
      "valid Loss: 0.2173 Acc: 0.9177\n",
      "Epoch 11/11\n",
      "----------\n",
      "train Loss: 0.2064 Acc: 0.9164\n",
      "valid Loss: 0.2166 Acc: 0.9186\n",
      "Training complete in 31m 27s\n",
      "Best valid Acc: 0.918862\n"
     ]
    }
   ],
   "source": [
    "model_list = [resnet18, resnet34, resnet50, resnet101, vgg16]\n",
    "model_name = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'vgg16']\n",
    "\n",
    "epochs = 12\n",
    "save_path = '/media/rene/Data/camelyon_out/tiles_224_100t/sample/models'\n",
    "results = []\n",
    "\n",
    "for idx, model_arch in enumerate(model_list):\n",
    "    model_ft = model_arch(pretrained=True)\n",
    "    if (model_arch==vgg16):\n",
    "        num_ftrs = model_ft.classifier[0].in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
    "    else:\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "    model_ft = model_ft.cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n",
    "\n",
    "    best_acc, model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=epochs)\n",
    "    results.append((model_name[idx], best_acc))\n",
    "    torch.save(model_ft.state_dict(), os.path.join(save_path, model_name[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, dataset_size, criterion):\n",
    "    model.train(False)  # Set model to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for data in dataloader:\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # for nets that have multiple outputs such as inception\n",
    "        if isinstance(outputs, tuple):\n",
    "            loss = sum((criterion(o,labels) for o in outputs))\n",
    "        else:\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_loss += loss.data[0] * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    del loss, outputs \n",
    "    \n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    epoch_acc = running_corrects / dataset_size\n",
    "    \n",
    "    print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  resnet18\n",
      "Loss: 0.1615 Acc: 0.9341\n",
      "model:  resnet34\n",
      "Loss: 0.1718 Acc: 0.9302\n",
      "model:  resnet50\n",
      "Loss: 0.1748 Acc: 0.9305\n",
      "model:  resnet101\n",
      "Loss: 0.1900 Acc: 0.9228\n",
      "model:  vgg16\n",
      "Loss: 0.2166 Acc: 0.9189\n"
     ]
    }
   ],
   "source": [
    "model_list = [resnet18, resnet34, resnet50, resnet101, vgg16]\n",
    "model_name = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'vgg16']    \n",
    "\n",
    "save_path = '/media/rene/Data/camelyon_out/tiles_224_100t/sample/models'\n",
    "dataloader = dataloaders['valid']\n",
    "dataset_size = dataset_sizes['valid']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for idx, model_arch in enumerate(model_list):\n",
    "    # get the proper model architecture\n",
    "    model_ft = model_arch(pretrained=True)\n",
    "    if (model_arch==vgg16):\n",
    "        num_ftrs = model_ft.classifier[0].in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
    "    else:\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "    model_ft = model_ft.cuda()\n",
    "    \n",
    "    # load the saved weights\n",
    "    model_ft.load_state_dict(torch.load(os.path.join(save_path, model_name[idx])))\n",
    "    print('model: ', model_name[idx])\n",
    "    eval_model(model_ft, dataloader, dataset_size, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fusion_model(model, model_list, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "        \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                inputs = Variable(inputs.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "                    \n",
    "                ######### Get model outputs\n",
    "                features = []\n",
    "                for model_tmp in model_list:\n",
    "                    output = model_tmp(inputs)\n",
    "                    features.append(output)\n",
    "                cat_features = torch.cat(features, 1)\n",
    "                    \n",
    "                ###########\n",
    "                    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(cat_features)\n",
    "\n",
    "                # for nets that have multiple outputs such as inception\n",
    "                if isinstance(outputs, tuple):\n",
    "                    loss = sum((criterion(o,labels) for o in outputs))\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0] * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print('saving model with acc ', epoch_acc)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best valid Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedSum, self).__init__()\n",
    "        self.fc1 = nn.Linear(5*2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        return out\n",
    "    \n",
    "fusion_model = WeightedSum()\n",
    "fusion_model = fusion_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2052 Acc: 0.9178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [13:12<52:48, 792.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.2068 Acc: 0.9189\n",
      "saving model with acc  0.9188622754491018\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.2011 Acc: 0.9190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [26:25<39:37, 792.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.2084 Acc: 0.9186\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.2002 Acc: 0.9181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [39:38<26:25, 792.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.2087 Acc: 0.9192\n",
      "saving model with acc  0.9191616766467066\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.1982 Acc: 0.9206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [52:50<13:12, 792.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.2069 Acc: 0.9213\n",
      "saving model with acc  0.9212574850299401\n",
      "Epoch 4/4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sz = 224\n",
    "\n",
    "model_list = [resnet18, resnet34, resnet50, resnet101, vgg16]\n",
    "model_name = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'vgg16']    \n",
    "save_path = '/media/rene/Data/camelyon_out/tiles_224_100t/sample/models'\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "PATH = '/media/rene/Data/camelyon_out/tiles_224_100t/sample'\n",
    "dataloaders, dataset_sizes = make_batch_gen(PATH, batch_size, num_workers)\n",
    "\n",
    "\n",
    "model_list_ft = []    \n",
    "for idx, model_arch in enumerate(model_list):\n",
    "    # get the proper model architecture\n",
    "    model_ft = model_arch(pretrained=True)\n",
    "    if (model_arch==vgg16):\n",
    "        num_ftrs = model_ft.classifier[0].in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
    "    else:\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "    model_ft = model_ft.cuda()\n",
    "    \n",
    "    # load the saved weights\n",
    "    model_ft.load_state_dict(torch.load(os.path.join(save_path, model_name[idx])))\n",
    "    model_list_ft.append(model_ft)\n",
    "    \n",
    "num_epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(fusion_model.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n",
    "\n",
    "\n",
    "model = train_fusion_model(fusion_model, model_list_ft, \n",
    "                    criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs)\n",
    "torch.save(model.state_dict(), os.path.join(save_path, 'weighted_sum_fusion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Perfromance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fusion_model(model, model_list, dataloader, dataset_size, criterion):\n",
    "    model.train(False)  # Set model to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for data in dataloader:\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "\n",
    "        \n",
    "        ######### Get model outputs\n",
    "        features = []\n",
    "        for model_tmp in model_list:\n",
    "            output = model_tmp(inputs)\n",
    "            features.append(output)\n",
    "        cat_features = torch.cat(features, 1)\n",
    "        ###########\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(cat_features)\n",
    "        \n",
    "        # for nets that have multiple outputs such as inception\n",
    "        if isinstance(outputs, tuple):\n",
    "            loss = sum((criterion(o,labels) for o in outputs))\n",
    "        else:\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_loss += loss.data[0] * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    del loss, outputs \n",
    "    \n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    epoch_acc = running_corrects / dataset_size\n",
    "    \n",
    "    print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2894 Acc: 0.8907\n"
     ]
    }
   ],
   "source": [
    "model_list = [resnet18, resnet34, resnet50, resnet101, vgg16]\n",
    "model_name = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'vgg16']    \n",
    "save_path = '/media/rene/Data/camelyon_out/tiles_224_100t/sample/models'\n",
    "\n",
    "model_list_ft = []    \n",
    "for idx, model_arch in enumerate(model_list):\n",
    "    # get the proper model architecture\n",
    "    model_ft = model_arch(pretrained=True)\n",
    "    if (model_arch==vgg16):\n",
    "        num_ftrs = model_ft.classifier[0].in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
    "    else:\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "    model_ft = model_ft.cuda()\n",
    "    \n",
    "    # load the saved weights\n",
    "    model_ft.load_state_dict(torch.load(os.path.join(save_path, model_name[idx])))\n",
    "    model_list_ft.append(model_ft)\n",
    "    \n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "PATH = '/media/rene/Data/camelyon_out/tiles_224_100t/sample'\n",
    "dataloaders, dataset_sizes = make_batch_gen(PATH, batch_size, num_workers)\n",
    "\n",
    "fusion_model = WeightedSum()\n",
    "fusion_model.load_state_dict(torch.load(os.path.join(save_path, 'weighted_sum_fusion')))\n",
    "fusion_model = fusion_model.cuda()\n",
    "\n",
    "dataloader = dataloaders['valid']\n",
    "dataset_size = dataset_sizes['valid']\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "eval_fusion_model(fusion_model, model_list_ft, dataloader, dataset_size, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
