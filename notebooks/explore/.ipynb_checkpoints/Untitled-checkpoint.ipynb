{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from openslide import OpenSlide, OpenSlideUnsupportedFormatError\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.sampler as sampler\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Base Directory where data is stored\n",
    "base_data_dir = '/media/rene/Data/CAMELYON16'\n",
    "base_out_dir = '/media/rene/Data/camelyon_out'\n",
    "\n",
    "nontumor_data_dir = '/media/rene/Data/CAMELYON16/TrainingData/Train_Normal'\n",
    "tumor_data_dir = '/media/rene/Data/CAMELYON16/TrainingData/Train_Tumor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "wsi.level_dimensions ((97792, 221184), (49152, 110592), (24576, 55296), (12288, 27648), (6144, 13824), (3072, 7168), (1536, 3584), (1024, 2048), (512, 1024), (512, 512))\n",
      "wsi.level_downsamples (1.0, 1.9947916666666665, 3.989583333333333, 7.979166666666666, 15.958333333333332, 31.345238095238095, 62.69047619047619, 101.75, 203.5, 311.5)\n"
     ]
    }
   ],
   "source": [
    "wsi_path = '/media/rene/Data/CAMELYON16/TrainingData/Train_Normal/Normal_001.tif'\n",
    "wsi = OpenSlide(wsi_path)\n",
    "print(wsi.level_count)\n",
    "print('wsi.level_dimensions', wsi.level_dimensions)\n",
    "print('wsi.level_downsamples', wsi.level_downsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't do anything to the entire image at once, so how is otsu's segmentation done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(wsi):\n",
    "    \"\"\"Downsample the wsi, convert to hsv, and then threshold it with otsu's method\"\"\"\n",
    "    level = 5\n",
    "    img = wsi.read_region(location=(0, 0), level=5, size=wsi_image.level_dimensions[level_used])\n",
    "    img = np.array(wsi.rgb_image_pil)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def convert_to_patches:\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
